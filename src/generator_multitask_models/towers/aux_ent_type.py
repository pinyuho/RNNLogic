import torch
import torch.nn as nn
import torch.nn.functional as F
import logging

class AuxEntTypeTower(nn.Module):
    def __init__(self, hidden_dim, type_size, soft_label=False): # soft_label: æ©Ÿç‡åˆ†ä½ˆ
        super().__init__()
        self.head = nn.Linear(hidden_dim, type_size)
        self.type_size = type_size

        self.soft_label = soft_label
        self.bce_loss = nn.BCEWithLogitsLoss(reduction="none")
        self.kl_loss  = nn.KLDivLoss(reduction="none")

    def forward(self, feature, target=None, mask=None, weight=None, alpha=0.0): # weight æ²’æœ‰ç”¨åˆ°ï¼Œå¿½ç•¥å³å¯
        # alpha   : 0 â†’ 100% teacher forcing; 1 â†’ 100% ç”¨ä¸Šä¸€æ­¥é æ¸¬

        # logits = self.head(feature)
        # if target is None or mask is None:
        #     return logits, None  # ğŸ‘‰ inference æ¨¡å¼åªå›å‚³ logits
    
        # per_elem_loss = self.criterion(logits, target)       # (B , L , T)
        # mask3d = mask.unsqueeze(-1).float()                  # (B , L , 1)
        # masked_loss = per_elem_loss * mask3d                 # padding â†’ 0
        # loss = masked_loss.sum() / mask3d.sum().clamp(min=1)
        # return logits.detach(), loss

        logits = self.head(feature)
        # â”€â”€â”€â”€â”€ æ¨è«–éšæ®µ â”€â”€â”€â”€â”€
        if target is None or mask is None:
            return logits, None

        B, L, T = logits.shape
        mask3d = mask.unsqueeze(-1).float()               # (B, L, 1)

        # â”€â”€â”€â”€â”€ æ··åˆ çœŸå¯¦ target è·Ÿ ä¸Šä¸€éƒ¨é æ¸¬å€¼ â”€â”€â”€â”€â”€
        with torch.no_grad():
            prev = target[:, 0]                           # t=0 ç”¨çœŸå€¼
            soft_buf = []
            for t in range(L):
                mix = (1 - alpha) * target[:, t] + alpha * prev # é€™ä¸€è¡Œï¼
                soft_buf.append(mix.unsqueeze(1))         # (B, 1, T)

                # æ›´æ–° prevï¼šå–ç•¶å‰é æ¸¬ï¼ˆsoftmax æˆ– sigmoid çš†å¯ï¼‰
                prev = (
                    F.softmax(logits[:, t], dim=-1) if self.soft_label
                    else torch.sigmoid(logits[:, t])
                ).detach()

            soft_target = torch.cat(soft_buf, dim=1)      # (B, L, T)

        # â”€â”€â”€â”€â”€ é¸æ“‡ loss â”€â”€â”€â”€â”€
        if self.soft_label:
            # soft-label cross-entropy = âˆ’ Î£ p * log q
            log_q    = F.log_softmax(logits, dim=-1)
            per_elem = -(soft_target * log_q)             # (B, L, T)
        else:
            per_elem = self.bce_loss(logits, soft_target) # (B, L, T)

        loss = (per_elem * mask3d).sum() / mask3d.sum().clamp(min=1.0)
        return logits.detach(), loss