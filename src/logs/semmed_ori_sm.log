W0501 17:52:42.368353 3004976 torch/distributed/run.py:793] 
W0501 17:52:42.368353 3004976 torch/distributed/run.py:793] *****************************************
W0501 17:52:42.368353 3004976 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0501 17:52:42.368353 3004976 torch/distributed/run.py:793] *****************************************
Data loading | DONE!
Data loading | DONE!
2025-05-01 17:52:59,066 INFO     -------------------------
2025-05-01 17:52:59,066 INFO     | Pre-train Generator
2025-05-01 17:52:59,066 INFO     -------------------------
mode:  ori
mode:  ori
2025-05-01 17:52:59,202 INFO     >>>>> Generator: Training
2025-05-01 17:53:03,751 INFO     1000 10000 2.617082
2025-05-01 17:53:07,462 INFO     2000 10000 2.524798
2025-05-01 17:53:11,473 INFO     3000 10000 2.511061
2025-05-01 17:53:15,264 INFO     4000 10000 2.499214
2025-05-01 17:53:19,774 INFO     5000 10000 2.498550
2025-05-01 17:53:23,569 INFO     6000 10000 2.493884
2025-05-01 17:53:27,297 INFO     7000 10000 2.491780
2025-05-01 17:53:31,389 INFO     8000 10000 2.484261
2025-05-01 17:53:35,121 INFO     9000 10000 2.486301
2025-05-01 17:53:39,145 INFO     10000 10000 2.474596
2025-05-01 17:53:39,149 INFO     -------------------------
2025-05-01 17:53:39,149 INFO     | EM Iteration: 1/1
2025-05-01 17:53:39,149 INFO     -------------------------
2025-05-01 17:53:39,149 INFO     >>>>> Generator: Rule generation with sampling
2025-05-01 17:53:39,350 INFO     Predictor: read 496 rules from list.
2025-05-01 17:53:39,350 INFO     Initializing distributed process group
2025-05-01 17:53:39,360 INFO     Predictor: read 496 rules from list.
2025-05-01 17:53:39,363 INFO     Preprocess training set
2025-05-01 17:53:39,441 INFO     >>>>> Predictor: Training
[rank1]:[W501 17:53:40.851239079 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W501 17:53:40.868948344 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
2025-05-01 17:54:22,882 INFO     100 29348 14.231636 16130632.0
2025-05-01 17:55:07,678 INFO     200 29348 14.119928 16070142.1
2025-05-01 17:55:50,974 INFO     300 29348 14.118111 16130632.0
2025-05-01 17:56:34,220 INFO     400 29348 13.933010 16130632.0
2025-05-01 17:57:16,488 INFO     500 29348 13.672813 16130632.0
2025-05-01 17:57:58,501 INFO     600 29348 13.725976 16009652.3
2025-05-01 17:58:42,802 INFO     700 29348 13.483242 16130632.0
2025-05-01 17:59:28,313 INFO     800 29348 13.520261 16130632.0
2025-05-01 18:00:11,449 INFO     900 29348 13.493156 16130632.0
2025-05-01 18:00:55,379 INFO     1000 29348 13.352287 16130632.0
2025-05-01 18:00:55,383 INFO     >>>>> Predictor: Evaluating on valid
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/run_rnnlogic.py", line 181, in <module>
[rank1]:     main(parse_args())
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/run_rnnlogic.py", line 107, in main
[rank1]:     valid_mrr_iter = solver_p.evaluate('valid', expectation=cfg.predictor.eval.expectation)
[rank1]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/trainer.py", line 173, in evaluate
[rank1]:     logits, mask = model(all_h, all_r, None)
[rank1]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/predictors.py", line 64, in forward
[rank1]:     x = self.graph.grounding(all_h, r_head, r_body, edges_to_remove)
[rank1]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/data.py", line 256, in grounding
[rank1]:     x = self.propagate(x, r_body, e2r_mb, chunk_size)
[rank1]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/data.py", line 213, in propagate
[rank1]:     out[:, start:end] = torch.sparse.mm(A, x[:, start:end])
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 1 has a total capacity of 11.72 GiB of which 9.81 MiB is free. Process 3005017 has 378.00 MiB memory in use. Including non-PyTorch memory, this process has 11.33 GiB memory in use. Of the allocated memory 10.85 GiB is allocated by PyTorch, and 124.59 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/run_rnnlogic.py", line 181, in <module>
[rank0]:     main(parse_args())
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/run_rnnlogic.py", line 107, in main
[rank0]:     valid_mrr_iter = solver_p.evaluate('valid', expectation=cfg.predictor.eval.expectation)
[rank0]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/trainer.py", line 173, in evaluate
[rank0]:     logits, mask = model(all_h, all_r, None)
[rank0]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/predictors.py", line 64, in forward
[rank0]:     x = self.graph.grounding(all_h, r_head, r_body, edges_to_remove)
[rank0]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/data.py", line 256, in grounding
[rank0]:     x = self.propagate(x, r_body, e2r_mb, chunk_size)
[rank0]:   File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/emma/emma/bilab_archive/polly/RNNLogic/src/data.py", line 213, in propagate
[rank0]:     out[:, start:end] = torch.sparse.mm(A, x[:, start:end])
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 62.00 MiB. GPU 0 has a total capacity of 11.72 GiB of which 51.88 MiB is free. Including non-PyTorch memory, this process has 11.34 GiB memory in use. Process 3005018 has 180.00 MiB memory in use. Of the allocated memory 10.86 GiB is allocated by PyTorch, and 142.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W0501 18:01:24.966360 3004976 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 3005017 closing signal SIGTERM
E0501 18:01:25.231350 3004976 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 1 (pid: 3005018) of binary: /home/emma/polly_env_py39/bin/python3.9
Traceback (most recent call last):
  File "/home/emma/polly_env_py39/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/emma/polly_env_py39/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run_rnnlogic.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-05-01_18:01:24
  host      : emma-System-Product-Name
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3005018)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
