W0507 03:47:26.412362 50166 torch/distributed/run.py:793] 
W0507 03:47:26.412362 50166 torch/distributed/run.py:793] *****************************************
W0507 03:47:26.412362 50166 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0507 03:47:26.412362 50166 torch/distributed/run.py:793] *****************************************
Data loading | DONE!
Data loading | DONE!
mode:  ori
2025-05-07 03:47:29,195 INFO     -------------------------
2025-05-07 03:47:29,195 INFO     | Pre-train Generator
2025-05-07 03:47:29,195 INFO     -------------------------
mode:  ori
2025-05-07 03:47:29,320 INFO     >>>>> Generator: Training
2025-05-07 03:47:34,101 INFO     1000 10000 2.112952
2025-05-07 03:47:38,067 INFO     2000 10000 2.064533
2025-05-07 03:47:41,986 INFO     3000 10000 2.056594
2025-05-07 03:47:45,863 INFO     4000 10000 2.049591
2025-05-07 03:47:49,771 INFO     5000 10000 2.044630
2025-05-07 03:47:53,658 INFO     6000 10000 2.041575
2025-05-07 03:47:57,551 INFO     7000 10000 2.037889
